{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5baaef8",
   "metadata": {},
   "source": [
    "# 02 - Spark ETL: CPU vs GPU (RAPIDS)\n",
    "\n",
    "This notebook provides comprehensive comparison of Spark ETL performance across compute backends:\n",
    "- **CPU Spark**: Traditional Spark with JVM-based processing\n",
    "- **GPU Spark (RAPIDS)**: RAPIDS cuDF with GPU acceleration\n",
    "- **Hybrid approaches**: Mixed CPU/GPU workflows\n",
    "\n",
    "## Workloads Tested\n",
    "1. **Data Ingestion**: Various formats (Parquet, CSV, ORC) at different scales\n",
    "2. **Filtering & Aggregation**: Common analytical operations\n",
    "3. **Joins**: Inner/outer joins with different data skew patterns\n",
    "4. **Complex Transformations**: Window functions, UDFs, complex queries\n",
    "5. **Data Export**: Writing results in different formats\n",
    "\n",
    "## Platform Compatibility\n",
    "- **Local Development**: Single-node testing\n",
    "- **Google Colab**: Free GPU tier testing\n",
    "- **AWS EMR**: Distributed GPU clusters\n",
    "- **SageMaker**: Unified development experience\n",
    "\n",
    "Results help determine optimal compute configuration for different ML data pipeline stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.bench.spark_utils import build_spark\n",
    "from src.bench.data_generator import create_synthetic_tabular\n",
    "\n",
    "# Ensure results directory exists\n",
    "Path('../results').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚ö° Spark ETL Benchmarks Setup\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Available memory: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "\n",
    "# Check environment for GPU availability\n",
    "gpu_available = False\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        print(f\"üéÆ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "    else:\n",
    "        print(\"üíª Running in CPU-only mode\")\n",
    "except ImportError:\n",
    "    print(\"üíª PyTorch not available, CPU-only mode\")\n",
    "\n",
    "# Check for cloud environment\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "is_sagemaker = os.path.exists('/opt/ml')\n",
    "is_emr = os.path.exists('/etc/hadoop')\n",
    "\n",
    "if is_colab:\n",
    "    print(\"‚òÅÔ∏è Google Colab environment detected\")\n",
    "elif is_sagemaker:\n",
    "    print(\"‚òÅÔ∏è AWS SageMaker environment detected\")\n",
    "elif is_emr:\n",
    "    print(\"‚òÅÔ∏è EMR cluster environment detected\")\n",
    "else:\n",
    "    print(\"üñ•Ô∏è Local development environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive test datasets for ETL benchmarking\n",
    "print(\"\\nüìä Preparing ETL Test Datasets\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create datasets of different sizes for scaling analysis\n",
    "dataset_configs = [\n",
    "    {'name': 'small', 'rows': 100_000, 'features': 50},\n",
    "    {'name': 'medium', 'rows': 1_000_000, 'features': 100},\n",
    "    {'name': 'large', 'rows': 5_000_000, 'features': 150} if not is_colab else {'name': 'large', 'rows': 1_000_000, 'features': 100}  # Adjust for Colab limits\n",
    "]\n",
    "\n",
    "etl_datasets = {}\n",
    "\n",
    "for config in dataset_configs:\n",
    "    print(f\"\\nüìà Creating {config['name']} dataset: {config['rows']:,} rows √ó {config['features']} features\")\n",
    "    \n",
    "    # Create dataset with ETL-friendly characteristics\n",
    "    files = create_synthetic_tabular(\n",
    "        n_rows=config['rows'],\n",
    "        n_features=config['features'],\n",
    "        n_categorical=min(20, config['features'] // 5),  # 20% categorical\n",
    "        output_dir=f\"../data/etl_{config['name']}\",\n",
    "        formats=['parquet', 'csv'],\n",
    "        add_skew=True  # Add realistic data skew for join testing\n",
    "    )\n",
    "    \n",
    "    etl_datasets[config['name']] = {\n",
    "        'config': config,\n",
    "        'files': files,\n",
    "        'parquet_path': files['parquet'],\n",
    "        'csv_path': files['csv']\n",
    "    }\n",
    "\n",
    "print(\"\\nüìã Dataset Summary:\")\n",
    "for name, info in etl_datasets.items():\n",
    "    parquet_size = Path(info['parquet_path']).stat().st_size / (1024**2)\n",
    "    csv_size = Path(info['csv_path']).stat().st_size / (1024**2)\n",
    "    print(f\"   {name.capitalize()}: Parquet {parquet_size:.1f}MB, CSV {csv_size:.1f}MB\")\n",
    "\n",
    "print(\"‚úÖ ETL datasets prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL Benchmark Functions\n",
    "print(\"\\nüîß Setting up ETL Benchmark Functions\")\n",
    "\n",
    "def benchmark_spark_operation(spark, operation_name: str, operation_func, *args, **kwargs):\n",
    "    \"\"\"Benchmark a Spark operation with timing and resource monitoring.\"\"\"\n",
    "    \n",
    "    # Get initial memory state\n",
    "    process = psutil.Process()\n",
    "    memory_before = process.memory_info().rss / (1024**2)  # MB\n",
    "    \n",
    "    # Run operation\n",
    "    start_time = time.perf_counter()\n",
    "    try:\n",
    "        result = operation_func(*args, **kwargs)\n",
    "        \n",
    "        # Force execution for lazy operations\n",
    "        if hasattr(result, 'count'):\n",
    "            count = result.count()\n",
    "        elif hasattr(result, 'collect'):\n",
    "            collected = result.collect()\n",
    "            count = len(collected)\n",
    "        else:\n",
    "            count = None\n",
    "            \n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        # Get memory after operation\n",
    "        memory_after = process.memory_info().rss / (1024**2)  # MB\n",
    "        \n",
    "        return {\n",
    "            'operation': operation_name,\n",
    "            'duration_seconds': end_time - start_time,\n",
    "            'memory_before_mb': memory_before,\n",
    "            'memory_after_mb': memory_after,\n",
    "            'memory_peak_mb': memory_after,  # Simplified\n",
    "            'result_count': count,\n",
    "            'success': True,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = time.perf_counter()\n",
    "        return {\n",
    "            'operation': operation_name,\n",
    "            'duration_seconds': end_time - start_time,\n",
    "            'memory_before_mb': memory_before,\n",
    "            'memory_after_mb': process.memory_info().rss / (1024**2),\n",
    "            'memory_peak_mb': process.memory_info().rss / (1024**2),\n",
    "            'result_count': None,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def create_etl_workloads(spark, dataset_path: str):\n",
    "    \"\"\"Create standardized ETL workloads for benchmarking.\"\"\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = spark.read.parquet(dataset_path)\n",
    "    df.cache()  # Cache for multiple operations\n",
    "    \n",
    "    workloads = {\n",
    "        'data_scan': lambda: df.count(),\n",
    "        \n",
    "        'simple_filter': lambda: df.filter(df.target == True).count(),\n",
    "        \n",
    "        'aggregation': lambda: df.groupBy('target').agg(\n",
    "            {'num_feat_0': 'avg', 'num_feat_1': 'sum', 'num_feat_2': 'max'}\n",
    "        ).collect(),\n",
    "        \n",
    "        'complex_filter': lambda: df.filter(\n",
    "            (df.num_feat_0 > 0) & \n",
    "            (df.target == True) & \n",
    "            df.cat_feat_0.isNotNull()\n",
    "        ).count(),\n",
    "        \n",
    "        'window_function': lambda: df.withColumn(\n",
    "            'row_number', \n",
    "            spark.sql.functions.row_number().over(\n",
    "                spark.sql.window.Window.partitionBy('target').orderBy('num_feat_0')\n",
    "            )\n",
    "        ).filter('row_number <= 100').count(),\n",
    "        \n",
    "        'join_operation': lambda: df.alias('a').join(\n",
    "            df.select('target', 'num_feat_0').alias('b'),\n",
    "            df.target == df.target,  # Self join for demonstration\n",
    "            'inner'\n",
    "        ).count()\n",
    "    }\n",
    "    \n",
    "    return workloads\n",
    "\n",
    "print(\"‚úÖ Benchmark functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cpu_benchmarks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU Spark Benchmarks\n",
    "print(\"\\nüíª Running CPU Spark Benchmarks\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cpu_results = []\n",
    "\n",
    "# Create CPU Spark session\n",
    "try:\n",
    "    spark_cpu = build_spark(\n",
    "        use_gpu=False, \n",
    "        app_name='CPUBenchmark',\n",
    "        extra_conf={\n",
    "            'spark.executor.memory': '4g',\n",
    "            'spark.driver.memory': '2g',\n",
    "            'spark.sql.adaptive.enabled': 'true',\n",
    "            'spark.sql.adaptive.coalescePartitions.enabled': 'true',\n",
    "            'spark.serializer': 'org.apache.spark.serializer.KryoSerializer'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ CPU Spark session created (version: {spark_cpu.version})\")\n",
    "    \n",
    "    # Benchmark each dataset\n",
    "    for dataset_name, dataset_info in etl_datasets.items():\n",
    "        print(f\"\\nüìä Benchmarking {dataset_name} dataset with CPU Spark...\")\n",
    "        \n",
    "        parquet_path = dataset_info['parquet_path']\n",
    "        workloads = create_etl_workloads(spark_cpu, parquet_path)\n",
    "        \n",
    "        for workload_name, workload_func in tqdm(workloads.items(), desc=f\"CPU {dataset_name}\"):\n",
    "            # Run each workload multiple times for stability\n",
    "            for run in range(3):\n",
    "                result = benchmark_spark_operation(\n",
    "                    spark_cpu, \n",
    "                    f\"{workload_name}_{dataset_name}\", \n",
    "                    workload_func\n",
    "                )\n",
    "                \n",
    "                result.update({\n",
    "                    'compute_backend': 'cpu',\n",
    "                    'dataset_name': dataset_name,\n",
    "                    'dataset_rows': dataset_info['config']['rows'],\n",
    "                    'dataset_features': dataset_info['config']['features'],\n",
    "                    'workload': workload_name,\n",
    "                    'run_number': run + 1,\n",
    "                    'spark_version': spark_cpu.version\n",
    "                })\n",
    "                \n",
    "                cpu_results.append(result)\n",
    "                \n",
    "                if not result['success']:\n",
    "                    print(f\"‚ö†Ô∏è Failed: {workload_name} on {dataset_name} - {result['error']}\")\n",
    "    \n",
    "    spark_cpu.stop()\n",
    "    print(\"\\n‚úÖ CPU benchmarks completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå CPU Spark benchmarks failed: {e}\")\n",
    "    cpu_results = []\n",
    "\n",
    "# Save CPU results\n",
    "if cpu_results:\n",
    "    cpu_df = pd.DataFrame(cpu_results)\n",
    "    cpu_df.to_csv('../results/spark_cpu_benchmarks.csv', index=False)\n",
    "    print(f\"üíæ Saved {len(cpu_results)} CPU benchmark results\")\n",
    "    \n",
    "    # Quick summary\n",
    "    successful_runs = cpu_df[cpu_df['success'] == True]\n",
    "    if not successful_runs.empty:\n",
    "        print(\"\\nüìä CPU Performance Summary:\")\n",
    "        summary = successful_runs.groupby(['dataset_name', 'workload'])['duration_seconds'].agg(['mean', 'std']).round(3)\n",
    "        print(summary.head(10))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No CPU results to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu_benchmarks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Spark (RAPIDS) Benchmarks\n",
    "print(\"\\nüéÆ Running GPU Spark (RAPIDS) Benchmarks\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "gpu_results = []\n",
    "\n",
    "if gpu_available:\n",
    "    try:\n",
    "        # Check for RAPIDS availability\n",
    "        rapids_available = False\n",
    "        try:\n",
    "            import cudf\n",
    "            rapids_available = True\n",
    "            print(f\"‚úÖ RAPIDS cuDF available: {cudf.__version__}\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è RAPIDS cuDF not available, using GPU Spark plugin simulation\")\n",
    "        \n",
    "        # Create GPU Spark session\n",
    "        gpu_conf = {\n",
    "            'spark.executor.memory': '4g',\n",
    "            'spark.driver.memory': '2g',\n",
    "            'spark.sql.adaptive.enabled': 'true'\n",
    "        }\n",
    "        \n",
    "        # Add RAPIDS configs if available\n",
    "        if rapids_available:\n",
    "            gpu_conf.update({\n",
    "                'spark.plugins': 'com.nvidia.spark.SQLPlugin',\n",
    "                'spark.rapids.sql.enabled': 'true',\n",
    "                'spark.rapids.memory.pinnedPool.size': '2G'\n",
    "            })\n",
    "        \n",
    "        spark_gpu = build_spark(\n",
    "            use_gpu=True,\n",
    "            app_name='GPUBenchmark',\n",
    "            extra_conf=gpu_conf\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ GPU Spark session created (version: {spark_gpu.version})\")\n",
    "        \n",
    "        # Benchmark each dataset\n",
    "        for dataset_name, dataset_info in etl_datasets.items():\n",
    "            print(f\"\\nüöÄ Benchmarking {dataset_name} dataset with GPU Spark...\")\n",
    "            \n",
    "            parquet_path = dataset_info['parquet_path']\n",
    "            workloads = create_etl_workloads(spark_gpu, parquet_path)\n",
    "            \n",
    "            for workload_name, workload_func in tqdm(workloads.items(), desc=f\"GPU {dataset_name}\"):\n",
    "                # Run each workload multiple times for stability\n",
    "                for run in range(3):\n",
    "                    result = benchmark_spark_operation(\n",
    "                        spark_gpu,\n",
    "                        f\"{workload_name}_{dataset_name}\",\n",
    "                        workload_func\n",
    "                    )\n",
    "                    \n",
    "                    result.update({\n",
    "                        'compute_backend': 'gpu' if rapids_available else 'gpu_simulated',\n",
    "                        'dataset_name': dataset_name,\n",
    "                        'dataset_rows': dataset_info['config']['rows'],\n",
    "                        'dataset_features': dataset_info['config']['features'],\n",
    "                        'workload': workload_name,\n",
    "                        'run_number': run + 1,\n",
    "                        'spark_version': spark_gpu.version,\n",
    "                        'rapids_enabled': rapids_available\n",
    "                    })\n",
    "                    \n",
    "                    gpu_results.append(result)\n",
    "                    \n",
    "                    if not result['success']:\n",
    "                        print(f\"‚ö†Ô∏è Failed: {workload_name} on {dataset_name} - {result['error']}\")\n",
    "        \n",
    "        spark_gpu.stop()\n",
    "        print(\"\\n‚úÖ GPU benchmarks completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GPU Spark benchmarks failed: {e}\")\n",
    "        gpu_results = []\n",
    "\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No GPU available, skipping GPU benchmarks\")\n",
    "    print(\"   üí° To test GPU acceleration:\")\n",
    "    print(\"      - Use Google Colab with GPU runtime\")\n",
    "    print(\"      - Deploy to AWS EMR with GPU instances\")\n",
    "    print(\"      - Use local machine with NVIDIA GPU\")\n",
    "\n",
    "# Save GPU results\n",
    "if gpu_results:\n",
    "    gpu_df = pd.DataFrame(gpu_results)\n",
    "    gpu_df.to_csv('../results/spark_gpu_benchmarks.csv', index=False)\n",
    "    print(f\"üíæ Saved {len(gpu_results)} GPU benchmark results\")\n",
    "    \n",
    "    # Quick summary\n",
    "    successful_runs = gpu_df[gpu_df['success'] == True]\n",
    "    if not successful_runs.empty:\n",
    "        print(\"\\nüìä GPU Performance Summary:\")\n",
    "        summary = successful_runs.groupby(['dataset_name', 'workload'])['duration_seconds'].agg(['mean', 'std']).round(3)\n",
    "        print(summary.head(10))\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No GPU results to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Comparison Benchmarks (Parquet vs CSV)\n",
    "print(\"\\nüìã Running Format Comparison Benchmarks\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "format_results = []\n",
    "\n",
    "try:\n",
    "    # Create a fresh Spark session for format comparison\n",
    "    spark_format = build_spark(\n",
    "        use_gpu=False,\n",
    "        app_name='FormatComparison',\n",
    "        extra_conf={\n",
    "            'spark.executor.memory': '4g',\n",
    "            'spark.driver.memory': '2g'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Format comparison Spark session created\")\n",
    "    \n",
    "    # Test different formats for each dataset\n",
    "    for dataset_name, dataset_info in etl_datasets.items():\n",
    "        print(f\"\\nüìä Comparing formats for {dataset_name} dataset...\")\n",
    "        \n",
    "        formats_to_test = {\n",
    "            'parquet': dataset_info['parquet_path'],\n",
    "            'csv': dataset_info['csv_path']\n",
    "        }\n",
    "        \n",
    "        for format_name, file_path in formats_to_test.items():\n",
    "            \n",
    "            # Read operation\n",
    "            def read_operation():\n",
    "                if format_name == 'parquet':\n",
    "                    return spark_format.read.parquet(file_path)\n",
    "                elif format_name == 'csv':\n",
    "                    return spark_format.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "            \n",
    "            # Benchmark read performance\n",
    "            for run in range(3):\n",
    "                result = benchmark_spark_operation(\n",
    "                    spark_format,\n",
    "                    f\"read_{format_name}_{dataset_name}\",\n",
    "                    lambda: read_operation().count()\n",
    "                )\n",
    "                \n",
    "                # Add file size information\n",
    "                file_size_mb = Path(file_path).stat().st_size / (1024**2)\n",
    "                \n",
    "                result.update({\n",
    "                    'operation_type': 'read',\n",
    "                    'format': format_name,\n",
    "                    'dataset_name': dataset_name,\n",
    "                    'dataset_rows': dataset_info['config']['rows'],\n",
    "                    'file_size_mb': file_size_mb,\n",
    "                    'run_number': run + 1,\n",
    "                    'throughput_mb_s': file_size_mb / result['duration_seconds'] if result['duration_seconds'] > 0 else 0\n",
    "                })\n",
    "                \n",
    "                format_results.append(result)\n",
    "            \n",
    "            print(f\"   {format_name.upper()}: {file_size_mb:.1f} MB\")\n",
    "    \n",
    "    spark_format.stop()\n",
    "    print(\"\\n‚úÖ Format comparison completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Format comparison failed: {e}\")\n",
    "    format_results = []\n",
    "\n",
    "# Save format results\n",
    "if format_results:\n",
    "    format_df = pd.DataFrame(format_results)\n",
    "    format_df.to_csv('../results/spark_format_comparison.csv', index=False)\n",
    "    print(f\"üíæ Saved {len(format_results)} format comparison results\")\n",
    "    \n",
    "    # Format comparison summary\n",
    "    successful_runs = format_df[format_df['success'] == True]\n",
    "    if not successful_runs.empty:\n",
    "        print(\"\\nüìä Format Performance Summary:\")\n",
    "        summary = successful_runs.groupby(['dataset_name', 'format']).agg({\n",
    "            'duration_seconds': ['mean', 'std'],\n",
    "            'throughput_mb_s': 'mean',\n",
    "            'file_size_mb': 'first'\n",
    "        }).round(3)\n",
    "        print(summary)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No format comparison results to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Analysis and Visualization\n",
    "print(\"\\nüìà Analysis and Visualization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Combine all results\n",
    "all_spark_results = []\n",
    "\n",
    "if cpu_results:\n",
    "    all_spark_results.extend(cpu_results)\n",
    "if gpu_results:\n",
    "    all_spark_results.extend(gpu_results)\n",
    "\n",
    "if all_spark_results:\n",
    "    combined_df = pd.DataFrame(all_spark_results)\n",
    "    combined_df.to_csv('../results/spark_etl_combined.csv', index=False)\n",
    "    \n",
    "    # Filter successful runs for analysis\n",
    "    successful_df = combined_df[combined_df['success'] == True].copy()\n",
    "    \n",
    "    if not successful_df.empty:\n",
    "        # Create comprehensive visualizations\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Spark ETL Performance Analysis', fontsize=16)\n",
    "        \n",
    "        # 1. CPU vs GPU Performance by Workload\n",
    "        if gpu_results and cpu_results:\n",
    "            perf_comparison = successful_df.groupby(['compute_backend', 'workload'])['duration_seconds'].mean().unstack()\n",
    "            perf_comparison.plot(kind='bar', ax=axes[0,0], width=0.8)\n",
    "            axes[0,0].set_title('CPU vs GPU Performance by Workload')\n",
    "            axes[0,0].set_ylabel('Duration (seconds)')\n",
    "            axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            axes[0,0].text(0.5, 0.5, 'CPU vs GPU\\nComparison\\nNot Available', \n",
    "                          ha='center', va='center', transform=axes[0,0].transAxes)\n",
    "        \n",
    "        # 2. Scaling by Dataset Size\n",
    "        scaling_data = successful_df.groupby(['dataset_name', 'compute_backend'])['duration_seconds'].mean().unstack()\n",
    "        if not scaling_data.empty:\n",
    "            scaling_data.plot(kind='bar', ax=axes[0,1], width=0.8)\n",
    "            axes[0,1].set_title('Performance Scaling by Dataset Size')\n",
    "            axes[0,1].set_ylabel('Duration (seconds)')\n",
    "            axes[0,1].legend()\n",
    "            axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 3. Memory Usage Comparison\n",
    "        memory_data = successful_df.groupby(['compute_backend', 'dataset_name'])['memory_peak_mb'].mean().unstack()\n",
    "        if not memory_data.empty:\n",
    "            memory_data.plot(kind='bar', ax=axes[0,2], width=0.8)\n",
    "            axes[0,2].set_title('Peak Memory Usage')\n",
    "            axes[0,2].set_ylabel('Memory (MB)')\n",
    "            axes[0,2].legend()\n",
    "            axes[0,2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 4. Workload Performance Breakdown\n",
    "        workload_perf = successful_df.groupby('workload')['duration_seconds'].agg(['mean', 'std'])\n",
    "        workload_perf['mean'].plot(kind='bar', ax=axes[1,0], yerr=workload_perf['std'], capsize=4)\n",
    "        axes[1,0].set_title('Workload Performance Breakdown')\n",
    "        axes[1,0].set_ylabel('Duration (seconds)')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 5. Format Comparison (if available)\n",
    "        if format_results:\n",
    "            format_successful = pd.DataFrame([r for r in format_results if r['success']])\n",
    "            if not format_successful.empty:\n",
    "                format_perf = format_successful.groupby(['dataset_name', 'format'])['throughput_mb_s'].mean().unstack()\n",
    "                format_perf.plot(kind='bar', ax=axes[1,1], width=0.8)\n",
    "                axes[1,1].set_title('Format Read Throughput')\n",
    "                axes[1,1].set_ylabel('Throughput (MB/s)')\n",
    "                axes[1,1].legend()\n",
    "                axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            axes[1,1].text(0.5, 0.5, 'Format\\nComparison\\nNot Available', \n",
    "                          ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "        \n",
    "        # 6. Efficiency Analysis (Operations per second)\n",
    "        successful_df['ops_per_second'] = successful_df['result_count'] / successful_df['duration_seconds']\n",
    "        efficiency_data = successful_df.groupby(['compute_backend', 'workload'])['ops_per_second'].mean().unstack()\n",
    "        if not efficiency_data.empty:\n",
    "            efficiency_data.plot(kind='bar', ax=axes[1,2], width=0.8)\n",
    "            axes[1,2].set_title('Processing Efficiency (Ops/sec)')\n",
    "            axes[1,2].set_ylabel('Operations per Second')\n",
    "            axes[1,2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            axes[1,2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../results/spark_etl_analysis.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nüìä PERFORMANCE SUMMARY:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Overall statistics\n",
    "        overall_stats = successful_df.groupby('compute_backend').agg({\n",
    "            'duration_seconds': ['mean', 'std', 'min', 'max'],\n",
    "            'memory_peak_mb': ['mean', 'max'],\n",
    "            'result_count': 'sum'\n",
    "        })\n",
    "        print(\"\\nüî¢ Overall Statistics by Compute Backend:\")\n",
    "        print(overall_stats.round(3))\n",
    "        \n",
    "        # Best performers\n",
    "        fastest_by_workload = successful_df.loc[successful_df.groupby('workload')['duration_seconds'].idxmin()]\n",
    "        print(\"\\nüèÜ Fastest Configuration by Workload:\")\n",
    "        for _, row in fastest_by_workload.iterrows():\n",
    "            print(f\"   {row['workload']}: {row['compute_backend']} ({row['duration_seconds']:.3f}s)\")\n",
    "        \n",
    "        print(f\"\\nüìÑ Detailed results saved to ../results/spark_etl_*.csv\")\n",
    "        print(f\"üìà Visualization saved to ../results/spark_etl_analysis.png\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No successful benchmark runs to analyze\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No benchmark results available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Insights and Recommendations\n",
    "print(\"\\nüí° SPARK ETL INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "insights = []\n",
    "recommendations = []\n",
    "\n",
    "if all_spark_results:\n",
    "    successful_df = pd.DataFrame([r for r in all_spark_results if r['success']])\n",
    "    \n",
    "    if not successful_df.empty:\n",
    "        # CPU vs GPU Analysis\n",
    "        if 'gpu' in successful_df['compute_backend'].values and 'cpu' in successful_df['compute_backend'].values:\n",
    "            cpu_avg = successful_df[successful_df['compute_backend'] == 'cpu']['duration_seconds'].mean()\n",
    "            gpu_avg = successful_df[successful_df['compute_backend'] == 'gpu']['duration_seconds'].mean()\n",
    "            \n",
    "            if gpu_avg < cpu_avg:\n",
    "                speedup = cpu_avg / gpu_avg\n",
    "                insights.append(f\"üöÄ GPU acceleration shows {speedup:.1f}x speedup on average\")\n",
    "                recommendations.append(\"Consider GPU instances for large-scale ETL workloads\")\n",
    "            else:\n",
    "                slowdown = gpu_avg / cpu_avg\n",
    "                insights.append(f\"‚ö†Ô∏è GPU shows {slowdown:.1f}x slower performance - may need optimization\")\n",
    "                recommendations.append(\"Focus on CPU optimization or larger datasets for GPU benefits\")\n",
    "        \n",
    "        # Dataset scaling insights\n",
    "        if len(successful_df['dataset_name'].unique()) > 1:\n",
    "            scaling_analysis = successful_df.groupby(['dataset_name', 'workload'])['duration_seconds'].mean().unstack()\n",
    "            \n",
    "            # Check if performance scales linearly with data size\n",
    "            dataset_rows = {name: info['config']['rows'] for name, info in etl_datasets.items()}\n",
    "            \n",
    "            if 'small' in dataset_rows and 'large' in dataset_rows:\n",
    "                size_ratio = dataset_rows['large'] / dataset_rows['small']\n",
    "                time_ratio = successful_df[successful_df['dataset_name'] == 'large']['duration_seconds'].mean() / \\\n",
    "                           successful_df[successful_df['dataset_name'] == 'small']['duration_seconds'].mean()\n",
    "                \n",
    "                if time_ratio < size_ratio * 0.8:  # Better than linear scaling\n",
    "                    insights.append(f\"üìà Excellent scaling: {size_ratio:.1f}x data size ‚Üí {time_ratio:.1f}x time\")\n",
    "                    recommendations.append(\"System handles large datasets efficiently - consider batch processing\")\n",
    "                elif time_ratio > size_ratio * 1.5:  # Worse than linear scaling\n",
    "                    insights.append(f\"üìâ Poor scaling: {size_ratio:.1f}x data size ‚Üí {time_ratio:.1f}x time\")\n",
    "                    recommendations.append(\"Investigate memory bottlenecks and consider data partitioning\")\n",
    "                else:\n",
    "                    insights.append(f\"üìä Linear scaling: {size_ratio:.1f}x data size ‚Üí {time_ratio:.1f}x time\")\n",
    "        \n",
    "        # Memory usage insights\n",
    "        max_memory = successful_df['memory_peak_mb'].max()\n",
    "        avg_memory = successful_df['memory_peak_mb'].mean()\n",
    "        \n",
    "        if max_memory > 8000:  # > 8GB\n",
    "            insights.append(f\"üî¥ High memory usage detected: {max_memory:.0f} MB peak\")\n",
    "            recommendations.append(\"Consider larger instances or data partitioning strategies\")\n",
    "        elif avg_memory < 2000:  # < 2GB average\n",
    "            insights.append(f\"üü¢ Low memory usage: {avg_memory:.0f} MB average\")\n",
    "            recommendations.append(\"Current setup is memory-efficient - can handle larger datasets\")\n",
    "        \n",
    "        # Workload-specific insights\n",
    "        workload_times = successful_df.groupby('workload')['duration_seconds'].mean().sort_values(ascending=False)\n",
    "        slowest_workload = workload_times.index[0]\n",
    "        fastest_workload = workload_times.index[-1]\n",
    "        \n",
    "        insights.append(f\"üêå Slowest operation: {slowest_workload} ({workload_times.iloc[0]:.3f}s)\")\n",
    "        insights.append(f\"‚ö° Fastest operation: {fastest_workload} ({workload_times.iloc[-1]:.3f}s)\")\n",
    "        \n",
    "        if 'join_operation' in workload_times.index and workload_times['join_operation'] > workload_times.mean() * 2:\n",
    "            recommendations.append(\"Join operations are bottleneck - consider data skew mitigation\")\n",
    "        \n",
    "        if 'window_function' in workload_times.index and workload_times['window_function'] > workload_times.mean() * 1.5:\n",
    "            recommendations.append(\"Window functions are expensive - consider alternative approaches\")\n",
    "\n",
    "# Format comparison insights\n",
    "if format_results:\n",
    "    format_successful = pd.DataFrame([r for r in format_results if r['success']])\n",
    "    if not format_successful.empty:\n",
    "        format_throughput = format_successful.groupby('format')['throughput_mb_s'].mean()\n",
    "        \n",
    "        if 'parquet' in format_throughput.index and 'csv' in format_throughput.index:\n",
    "            parquet_speed = format_throughput['parquet']\n",
    "            csv_speed = format_throughput['csv']\n",
    "            \n",
    "            if parquet_speed > csv_speed * 2:\n",
    "                insights.append(f\"üì¶ Parquet is {parquet_speed/csv_speed:.1f}x faster than CSV\")\n",
    "                recommendations.append(\"Use Parquet format for production ETL pipelines\")\n",
    "            else:\n",
    "                insights.append(f\"üìÑ Parquet vs CSV performance difference is moderate ({parquet_speed/csv_speed:.1f}x)\")\n",
    "\n",
    "# Platform-specific recommendations\n",
    "platform_recommendations = []\n",
    "if is_colab:\n",
    "    platform_recommendations.extend([\n",
    "        \"Use Colab Pro for larger memory limits on big datasets\",\n",
    "        \"Mount Google Drive for persistent data storage\",\n",
    "        \"Consider BigQuery integration for massive datasets\"\n",
    "    ])\n",
    "elif is_sagemaker:\n",
    "    platform_recommendations.extend([\n",
    "        \"Use SageMaker Processing for distributed ETL jobs\",\n",
    "        \"Leverage S3 for scalable data storage\",\n",
    "        \"Consider EMR integration for larger Spark clusters\"\n",
    "    ])\n",
    "elif is_emr:\n",
    "    platform_recommendations.extend([\n",
    "        \"Optimize cluster size based on workload characteristics\",\n",
    "        \"Use spot instances for cost-effective processing\",\n",
    "        \"Enable dynamic allocation for variable workloads\"\n",
    "    ])\n",
    "else:\n",
    "    platform_recommendations.extend([\n",
    "        \"Consider cloud deployment for larger scale processing\",\n",
    "        \"Use local SSD storage for optimal I/O performance\",\n",
    "        \"Monitor resource utilization for capacity planning\"\n",
    "    ])\n",
    "\n",
    "# Print insights and recommendations\n",
    "print(\"\\nüéØ KEY INSIGHTS:\")\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"   {i}. {insight}\")\n",
    "\n",
    "print(\"\\nüîß OPTIMIZATION RECOMMENDATIONS:\")\n",
    "all_recommendations = recommendations + platform_recommendations\n",
    "for i, rec in enumerate(all_recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(\"\\nüåê PLATFORM-SPECIFIC DEPLOYMENT GUIDES:\")\n",
    "deployment_guides = {\n",
    "    \"Google Colab\": \"Use !pip install for dependencies, mount drive for data persistence\",\n",
    "    \"AWS SageMaker\": \"Use SageMaker Processing jobs for production ETL workflows\", \n",
    "    \"EMR Spark\": \"Configure cluster with appropriate instance types and auto-scaling\",\n",
    "    \"Local Development\": \"Use conda/venv for environment management, consider Docker for consistency\"\n",
    "}\n",
    "\n",
    "for platform, guide in deployment_guides.items():\n",
    "    print(f\"   üì± {platform}: {guide}\")\n",
    "\n",
    "print(\"\\n‚úÖ Spark ETL Benchmarks Complete!\")\n",
    "print(f\"üìä Check ../results/ for detailed performance data and analysis plots\")\n",
    "print(f\"üîó Next: Run notebook 03 for ML training pipeline benchmarks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
