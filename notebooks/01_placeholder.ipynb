{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2342d9a9",
   "metadata": {},
   "source": [
    "# 01 - I/O Microbenchmarks\n",
    "\n",
    "This notebook conducts comprehensive I/O performance analysis across different:\n",
    "- **Storage types**: NVMe, NFS, object stores, memory\n",
    "- **Access patterns**: Sequential vs random, different block sizes\n",
    "- **File characteristics**: Size, format, compression\n",
    "- **Concurrency levels**: Single vs multi-threaded access\n",
    "\n",
    "Results help understand storage bottlenecks in ML data pipelines and guide optimization decisions.\n",
    "\n",
    "## Experiment Matrix\n",
    "1. **Sequential Read Performance** across block sizes and file sizes\n",
    "2. **Random Access Patterns** with different seek behaviors\n",
    "3. **Format Comparison** (Parquet vs CSV vs binary formats)\n",
    "4. **Small File vs Large File** performance characteristics\n",
    "5. **Memory-mapped vs Direct I/O** comparisons\n",
    "6. **Multi-threaded Access** patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import mmap\n",
    "import json\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.bench.io_bench import seq_read_bench, rand_read_bench\n",
    "from src.bench.data_generator import create_synthetic_tabular, create_mixed_workload_data\n",
    "from src.bench.plotting import lineplot_csv\n",
    "\n",
    "# Ensure results directory exists\n",
    "Path('../results').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üîß I/O Microbenchmarks Setup\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Available memory: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "\n",
    "# Check if we need to generate test data\n",
    "data_dir = Path('../data')\n",
    "if not data_dir.exists() or len(list(data_dir.glob('**/*'))) < 10:\n",
    "    print(\"üìä Generating test data...\")\n",
    "    # Generate minimal test dataset\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create some test files of different sizes\n",
    "    test_sizes = [1024, 10*1024, 100*1024, 1024*1024, 10*1024*1024]  # 1KB to 10MB\n",
    "    for i, size in enumerate(test_sizes):\n",
    "        test_file = data_dir / f'test_{size//1024}kb.dat'\n",
    "        with open(test_file, 'wb') as f:\n",
    "            f.write(os.urandom(size))\n",
    "        print(f\"   Created {test_file} ({size/1024:.1f} KB)\")\n",
    "    \n",
    "    print(\"‚úÖ Test data generated\")\n",
    "else:\n",
    "    print(\"‚úÖ Using existing test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sequential_benchmarks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced sequential read benchmarks\n",
    "print(\"\\nüîÑ Running Sequential Read Benchmarks\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all test files\n",
    "test_files = [p for p in Path('../data').glob('**/*') if p.is_file() and p.stat().st_size > 0]\n",
    "print(f\"Found {len(test_files)} test files\")\n",
    "\n",
    "# Test different block sizes for sequential reads\n",
    "block_sizes_kb = [4, 16, 64, 256, 1024, 4096]  # 4KB to 4MB blocks\n",
    "seq_results = []\n",
    "\n",
    "for file_path in tqdm(test_files, desc=\"Files\"):\n",
    "    file_size = file_path.stat().st_size\n",
    "    \n",
    "    # Skip very small files for large block sizes\n",
    "    max_block_kb = min(4096, file_size // 1024)\n",
    "    relevant_blocks = [b for b in block_sizes_kb if b <= max_block_kb]\n",
    "    \n",
    "    for block_kb in relevant_blocks:\n",
    "        try:\n",
    "            # Run benchmark multiple times for stability\n",
    "            times = []\n",
    "            for run in range(3):  # 3 runs per configuration\n",
    "                result = seq_read_bench(str(file_path), block_kb=block_kb)\n",
    "                times.append(result['mb_s'])\n",
    "            \n",
    "            # Record statistics\n",
    "            seq_results.append({\n",
    "                'file_path': str(file_path.relative_to(Path('../data'))),\n",
    "                'file_size_mb': file_size / (1024**2),\n",
    "                'file_size_category': categorize_file_size(file_size),\n",
    "                'block_kb': block_kb,\n",
    "                'throughput_mb_s': np.mean(times),\n",
    "                'throughput_std': np.std(times),\n",
    "                'throughput_min': np.min(times),\n",
    "                'throughput_max': np.max(times),\n",
    "                'access_pattern': 'sequential'\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error with {file_path} @ {block_kb}KB: {e}\")\n",
    "\n",
    "def categorize_file_size(size_bytes):\n",
    "    \"\"\"Categorize file size for analysis.\"\"\"\n",
    "    if size_bytes < 1024**2:  # < 1MB\n",
    "        return 'small'\n",
    "    elif size_bytes < 10 * 1024**2:  # < 10MB\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "seq_df = pd.DataFrame(seq_results)\n",
    "if not seq_df.empty:\n",
    "    seq_df.to_csv('../results/io_sequential_detailed.csv', index=False)\n",
    "    print(f\"\\nüìä Sequential Read Results:\")\n",
    "    print(seq_df.groupby(['file_size_category', 'block_kb'])['throughput_mb_s'].agg(['mean', 'std', 'count']))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No sequential benchmark results collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_benchmarks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced random access benchmarks\n",
    "print(\"\\nüé≤ Running Random Access Benchmarks\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Focus on files large enough for meaningful random access\n",
    "large_files = [f for f in test_files if f.stat().st_size > 1024**2]  # > 1MB\n",
    "\n",
    "if not large_files:\n",
    "    print(\"Creating larger test file for random access...\")\n",
    "    large_test_file = Path('../data/large_random_test.dat')\n",
    "    with open(large_test_file, 'wb') as f:\n",
    "        # Create 50MB file\n",
    "        for chunk in range(50):\n",
    "            f.write(os.urandom(1024*1024))\n",
    "    large_files = [large_test_file]\n",
    "    print(f\"Created {large_test_file} (50 MB)\")\n",
    "\n",
    "rand_results = []\n",
    "block_sizes_kb = [4, 16, 64, 256]  # Smaller blocks for random access\n",
    "sample_counts = [100, 500, 1000]  # Different numbers of random accesses\n",
    "\n",
    "for file_path in tqdm(large_files, desc=\"Large files\"):\n",
    "    file_size = file_path.stat().st_size\n",
    "    \n",
    "    for block_kb in block_sizes_kb:\n",
    "        for n_samples in sample_counts:\n",
    "            try:\n",
    "                # Run random access benchmark\n",
    "                times = []\n",
    "                for run in range(3):  # Multiple runs for stability\n",
    "                    result = rand_read_bench(\n",
    "                        str(file_path), \n",
    "                        block_kb=block_kb, \n",
    "                        samples=n_samples\n",
    "                    )\n",
    "                    times.append(result['mb_s'])\n",
    "                \n",
    "                rand_results.append({\n",
    "                    'file_path': str(file_path.relative_to(Path('../data'))),\n",
    "                    'file_size_mb': file_size / (1024**2),\n",
    "                    'block_kb': block_kb,\n",
    "                    'n_samples': n_samples,\n",
    "                    'throughput_mb_s': np.mean(times),\n",
    "                    'throughput_std': np.std(times),\n",
    "                    'iops': (n_samples * 3) / sum([seq_read_bench(str(file_path), block_kb)['seconds'] for _ in range(3)]),\n",
    "                    'access_pattern': 'random'\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Random access error {file_path} @ {block_kb}KB/{n_samples}: {e}\")\n",
    "\n",
    "rand_df = pd.DataFrame(rand_results)\n",
    "if not rand_df.empty:\n",
    "    rand_df.to_csv('../results/io_random_detailed.csv', index=False)\n",
    "    print(f\"\\nüìä Random Access Results:\")\n",
    "    print(rand_df.groupby(['block_kb', 'n_samples'])['throughput_mb_s'].agg(['mean', 'std']))\n",
    "    print(f\"\\nüíæ IOPS Results:\")\n",
    "    print(rand_df.groupby(['block_kb'])['iops'].agg(['mean', 'std']))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No random access results collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_mapped_benchmarks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-mapped file benchmarks\n",
    "print(\"\\nüó∫Ô∏è Running Memory-Mapped File Benchmarks\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def mmap_read_bench(file_path: str, access_pattern: str = 'sequential', n_accesses: int = 1000):\n",
    "    \"\"\"Benchmark memory-mapped file access.\"\"\"\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            total_bytes = 0\n",
    "            \n",
    "            if access_pattern == 'sequential':\n",
    "                # Sequential scan\n",
    "                block_size = 64 * 1024  # 64KB blocks\n",
    "                for i in range(0, len(mm), block_size):\n",
    "                    data = mm[i:i+block_size]\n",
    "                    total_bytes += len(data)\n",
    "            \n",
    "            elif access_pattern == 'random':\n",
    "                # Random access\n",
    "                block_size = 4 * 1024  # 4KB blocks\n",
    "                np.random.seed(42)\n",
    "                for _ in range(n_accesses):\n",
    "                    offset = np.random.randint(0, max(len(mm) - block_size, 1))\n",
    "                    data = mm[offset:offset+block_size]\n",
    "                    total_bytes += len(data)\n",
    "    \n",
    "    duration = time.perf_counter() - start_time\n",
    "    throughput_mb_s = (total_bytes / (1024**2)) / duration\n",
    "    \n",
    "    return {\n",
    "        'throughput_mb_s': throughput_mb_s,\n",
    "        'duration': duration,\n",
    "        'total_bytes': total_bytes,\n",
    "        'file_size': file_size\n",
    "    }\n",
    "\n",
    "mmap_results = []\n",
    "\n",
    "for file_path in tqdm(large_files, desc=\"mmap benchmarks\"):\n",
    "    file_size = file_path.stat().st_size\n",
    "    \n",
    "    for pattern in ['sequential', 'random']:\n",
    "        try:\n",
    "            times = []\n",
    "            for run in range(3):\n",
    "                result = mmap_read_bench(str(file_path), access_pattern=pattern)\n",
    "                times.append(result['throughput_mb_s'])\n",
    "            \n",
    "            mmap_results.append({\n",
    "                'file_path': str(file_path.relative_to(Path('../data'))),\n",
    "                'file_size_mb': file_size / (1024**2),\n",
    "                'access_pattern': f'mmap_{pattern}',\n",
    "                'throughput_mb_s': np.mean(times),\n",
    "                'throughput_std': np.std(times),\n",
    "                'method': 'memory_mapped'\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è mmap error {file_path} ({pattern}): {e}\")\n",
    "\n",
    "mmap_df = pd.DataFrame(mmap_results)\n",
    "if not mmap_df.empty:\n",
    "    mmap_df.to_csv('../results/io_mmap_detailed.csv', index=False)\n",
    "    print(f\"\\nüìä Memory-Mapped Results:\")\n",
    "    print(mmap_df.groupby('access_pattern')['throughput_mb_s'].agg(['mean', 'std']))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No memory-mapped results collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concurrent_benchmarks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-threaded I/O benchmarks\n",
    "print(\"\\nüîÄ Running Multi-threaded I/O Benchmarks\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def threaded_read_worker(args):\n",
    "    \"\"\"Worker function for multi-threaded reads.\"\"\"\n",
    "    file_path, block_kb, thread_id = args\n",
    "    try:\n",
    "        result = seq_read_bench(file_path, block_kb=block_kb)\n",
    "        result['thread_id'] = thread_id\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'thread_id': thread_id}\n",
    "\n",
    "def benchmark_concurrent_reads(file_paths, n_threads, block_kb=1024):\n",
    "    \"\"\"Benchmark concurrent reads across multiple threads.\"\"\"\n",
    "    # Prepare arguments for thread pool\n",
    "    args_list = []\n",
    "    for i in range(n_threads):\n",
    "        file_path = file_paths[i % len(file_paths)]  # Cycle through files\n",
    "        args_list.append((str(file_path), block_kb, i))\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Run concurrent reads\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "        future_to_args = {executor.submit(threaded_read_worker, args): args for args in args_list}\n",
    "        \n",
    "        for future in as_completed(future_to_args):\n",
    "            result = future.result()\n",
    "            if 'error' not in result:\n",
    "                results.append(result)\n",
    "    \n",
    "    total_time = time.perf_counter() - start_time\n",
    "    \n",
    "    return {\n",
    "        'n_threads': n_threads,\n",
    "        'total_time': total_time,\n",
    "        'individual_results': results,\n",
    "        'aggregate_throughput': sum(r['mb_s'] for r in results),\n",
    "        'avg_per_thread': np.mean([r['mb_s'] for r in results]) if results else 0\n",
    "    }\n",
    "\n",
    "# Test different thread counts\n",
    "thread_counts = [1, 2, 4, 8, min(16, psutil.cpu_count())]\n",
    "concurrent_results = []\n",
    "\n",
    "# Use a subset of files for concurrent testing\n",
    "test_files_subset = test_files[:min(8, len(test_files))]\n",
    "\n",
    "for n_threads in tqdm(thread_counts, desc=\"Thread counts\"):\n",
    "    try:\n",
    "        # Run multiple times for stability\n",
    "        thread_times = []\n",
    "        for run in range(3):\n",
    "            result = benchmark_concurrent_reads(test_files_subset, n_threads)\n",
    "            thread_times.append(result['aggregate_throughput'])\n",
    "        \n",
    "        concurrent_results.append({\n",
    "            'n_threads': n_threads,\n",
    "            'aggregate_throughput_mb_s': np.mean(thread_times),\n",
    "            'throughput_std': np.std(thread_times),\n",
    "            'scaling_efficiency': np.mean(thread_times) / (thread_times[0] if thread_counts[0] == 1 else np.mean(thread_times)) if n_threads > 1 else 1.0,\n",
    "            'method': 'concurrent_reads'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Concurrent benchmark error @ {n_threads} threads: {e}\")\n",
    "\n",
    "concurrent_df = pd.DataFrame(concurrent_results)\n",
    "if not concurrent_df.empty:\n",
    "    concurrent_df.to_csv('../results/io_concurrent_detailed.csv', index=False)\n",
    "    print(f\"\\nüìä Concurrent Read Results:\")\n",
    "    print(concurrent_df[['n_threads', 'aggregate_throughput_mb_s', 'scaling_efficiency']])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No concurrent benchmark results collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis and visualization\n",
    "print(\"\\nüìà Analysis and Visualization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Combine all results for comprehensive analysis\n",
    "all_results = []\n",
    "\n",
    "# Add sequential results\n",
    "if not seq_df.empty:\n",
    "    seq_summary = seq_df.copy()\n",
    "    seq_summary['benchmark_type'] = 'sequential'\n",
    "    all_results.append(seq_summary)\n",
    "\n",
    "# Add random results\n",
    "if not rand_df.empty:\n",
    "    rand_summary = rand_df.copy()\n",
    "    rand_summary['benchmark_type'] = 'random'\n",
    "    all_results.append(rand_summary)\n",
    "\n",
    "# Add memory-mapped results\n",
    "if not mmap_df.empty:\n",
    "    mmap_summary = mmap_df.copy()\n",
    "    mmap_summary['benchmark_type'] = 'memory_mapped'\n",
    "    all_results.append(mmap_summary)\n",
    "\n",
    "if all_results:\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    combined_df.to_csv('../results/io_benchmarks_combined.csv', index=False)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('I/O Performance Microbenchmarks', fontsize=16)\n",
    "    \n",
    "    # 1. Sequential throughput by block size\n",
    "    if not seq_df.empty:\n",
    "        seq_by_block = seq_df.groupby('block_kb')['throughput_mb_s'].agg(['mean', 'std']).reset_index()\n",
    "        axes[0,0].errorbar(seq_by_block['block_kb'], seq_by_block['mean'], \n",
    "                          yerr=seq_by_block['std'], marker='o', capsize=5)\n",
    "        axes[0,0].set_xlabel('Block Size (KB)')\n",
    "        axes[0,0].set_ylabel('Throughput (MB/s)')\n",
    "        axes[0,0].set_title('Sequential Read Performance')\n",
    "        axes[0,0].set_xscale('log', base=2)\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Random vs Sequential comparison\n",
    "    if not rand_df.empty and not seq_df.empty:\n",
    "        # Get comparable data points\n",
    "        common_blocks = set(seq_df['block_kb']).intersection(set(rand_df['block_kb']))\n",
    "        if common_blocks:\n",
    "            seq_comp = seq_df[seq_df['block_kb'].isin(common_blocks)].groupby('block_kb')['throughput_mb_s'].mean()\n",
    "            rand_comp = rand_df[rand_df['block_kb'].isin(common_blocks)].groupby('block_kb')['throughput_mb_s'].mean()\n",
    "            \n",
    "            x = np.arange(len(common_blocks))\n",
    "            width = 0.35\n",
    "            \n",
    "            axes[0,1].bar(x - width/2, seq_comp.values, width, label='Sequential', alpha=0.8)\n",
    "            axes[0,1].bar(x + width/2, rand_comp.values, width, label='Random', alpha=0.8)\n",
    "            axes[0,1].set_xlabel('Block Size (KB)')\n",
    "            axes[0,1].set_ylabel('Throughput (MB/s)')\n",
    "            axes[0,1].set_title('Sequential vs Random Access')\n",
    "            axes[0,1].set_xticks(x)\n",
    "            axes[0,1].set_xticklabels([f'{int(b)}' for b in sorted(common_blocks)])\n",
    "            axes[0,1].legend()\n",
    "            axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. File size impact\n",
    "    if not seq_df.empty and 'file_size_category' in seq_df.columns:\n",
    "        size_impact = seq_df.groupby('file_size_category')['throughput_mb_s'].agg(['mean', 'std']).reset_index()\n",
    "        axes[1,0].bar(size_impact['file_size_category'], size_impact['mean'], \n",
    "                     yerr=size_impact['std'], capsize=5, alpha=0.8)\n",
    "        axes[1,0].set_xlabel('File Size Category')\n",
    "        axes[1,0].set_ylabel('Throughput (MB/s)')\n",
    "        axes[1,0].set_title('Throughput by File Size')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Concurrent scaling\n",
    "    if not concurrent_df.empty:\n",
    "        axes[1,1].plot(concurrent_df['n_threads'], concurrent_df['aggregate_throughput_mb_s'], \n",
    "                      marker='o', linewidth=2, markersize=8)\n",
    "        axes[1,1].set_xlabel('Number of Threads')\n",
    "        axes[1,1].set_ylabel('Aggregate Throughput (MB/s)')\n",
    "        axes[1,1].set_title('Multi-threaded Scaling')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add ideal scaling line\n",
    "        if len(concurrent_df) > 1:\n",
    "            baseline = concurrent_df[concurrent_df['n_threads'] == 1]['aggregate_throughput_mb_s'].iloc[0]\n",
    "            ideal_line = [baseline * t for t in concurrent_df['n_threads']]\n",
    "            axes[1,1].plot(concurrent_df['n_threads'], ideal_line, '--', alpha=0.5, label='Ideal scaling')\n",
    "            axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/io_benchmarks_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä BENCHMARK SUMMARY:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if not seq_df.empty:\n",
    "        best_seq = seq_df.loc[seq_df['throughput_mb_s'].idxmax()]\n",
    "        print(f\"üèÜ Best Sequential: {best_seq['throughput_mb_s']:.1f} MB/s @ {best_seq['block_kb']}KB blocks\")\n",
    "    \n",
    "    if not rand_df.empty:\n",
    "        best_rand = rand_df.loc[rand_df['throughput_mb_s'].idxmax()]\n",
    "        print(f\"üé≤ Best Random: {best_rand['throughput_mb_s']:.1f} MB/s @ {best_rand['block_kb']}KB blocks\")\n",
    "    \n",
    "    if not concurrent_df.empty:\n",
    "        best_concurrent = concurrent_df.loc[concurrent_df['aggregate_throughput_mb_s'].idxmax()]\n",
    "        print(f\"üîÄ Best Concurrent: {best_concurrent['aggregate_throughput_mb_s']:.1f} MB/s @ {best_concurrent['n_threads']} threads\")\n",
    "    \n",
    "    print(f\"\\nüìÑ Results saved to ../results/io_benchmarks_*.csv\")\n",
    "    print(f\"üìà Visualization saved to ../results/io_benchmarks_analysis.png\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No benchmark results to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance recommendations and insights\n",
    "print(\"\\nüí° PERFORMANCE INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Block size analysis\n",
    "if not seq_df.empty:\n",
    "    optimal_block = seq_df.loc[seq_df['throughput_mb_s'].idxmax(), 'block_kb']\n",
    "    insights.append(f\"üîß Optimal block size for sequential reads: {optimal_block}KB\")\n",
    "    \n",
    "    # Check if larger blocks are consistently better\n",
    "    block_perf = seq_df.groupby('block_kb')['throughput_mb_s'].mean().sort_index()\n",
    "    if len(block_perf) > 1:\n",
    "        if block_perf.iloc[-1] > block_perf.iloc[0] * 1.5:\n",
    "            insights.append(\"üìà Larger block sizes show significant performance gains\")\n",
    "        else:\n",
    "            insights.append(\"üìä Block size impact is moderate - other factors may dominate\")\n",
    "\n",
    "# Random vs Sequential analysis\n",
    "if not rand_df.empty and not seq_df.empty:\n",
    "    avg_seq = seq_df['throughput_mb_s'].mean()\n",
    "    avg_rand = rand_df['throughput_mb_s'].mean()\n",
    "    ratio = avg_seq / avg_rand\n",
    "    \n",
    "    if ratio > 10:\n",
    "        insights.append(f\"‚ö° Sequential access is {ratio:.1f}x faster - prioritize sequential data layouts\")\n",
    "    elif ratio > 3:\n",
    "        insights.append(f\"üìã Sequential access advantage ({ratio:.1f}x) - consider data organization\")\n",
    "    else:\n",
    "        insights.append(f\"üîç Random access penalty is moderate ({ratio:.1f}x) - storage may have good random performance\")\n",
    "\n",
    "# Concurrency analysis\n",
    "if not concurrent_df.empty and len(concurrent_df) > 1:\n",
    "    single_thread = concurrent_df[concurrent_df['n_threads'] == 1]['aggregate_throughput_mb_s'].iloc[0]\n",
    "    max_threads_row = concurrent_df.loc[concurrent_df['aggregate_throughput_mb_s'].idxmax()]\n",
    "    max_throughput = max_threads_row['aggregate_throughput_mb_s']\n",
    "    optimal_threads = max_threads_row['n_threads']\n",
    "    scaling = max_throughput / single_thread\n",
    "    \n",
    "    if scaling > optimal_threads * 0.8:  # Good scaling\n",
    "        insights.append(f\"üöÄ Excellent scaling: {scaling:.1f}x speedup with {optimal_threads} threads\")\n",
    "    elif scaling > optimal_threads * 0.5:  # Moderate scaling  \n",
    "        insights.append(f\"üìà Moderate scaling: {scaling:.1f}x speedup with {optimal_threads} threads\")\n",
    "    else:  # Poor scaling\n",
    "        insights.append(f\"‚ö†Ô∏è Limited scaling: {scaling:.1f}x speedup - I/O may be bottleneck\")\n",
    "\n",
    "# Memory-mapped analysis\n",
    "if not mmap_df.empty:\n",
    "    mmap_seq = mmap_df[mmap_df['access_pattern'] == 'mmap_sequential']['throughput_mb_s'].mean()\n",
    "    if not seq_df.empty:\n",
    "        regular_seq = seq_df['throughput_mb_s'].mean()\n",
    "        mmap_advantage = mmap_seq / regular_seq\n",
    "        \n",
    "        if mmap_advantage > 1.2:\n",
    "            insights.append(f\"üó∫Ô∏è Memory mapping shows {mmap_advantage:.1f}x advantage - consider for large files\")\n",
    "        elif mmap_advantage < 0.8:\n",
    "            insights.append(f\"üìÅ Regular file I/O outperforms memory mapping - system may have good page cache\")\n",
    "        else:\n",
    "            insights.append(f\"‚öñÔ∏è Memory mapping and regular I/O perform similarly\")\n",
    "\n",
    "# General recommendations\n",
    "print(\"\\nüéØ KEY INSIGHTS:\")\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"   {i}. {insight}\")\n",
    "\n",
    "print(\"\\nüîß OPTIMIZATION RECOMMENDATIONS:\")\n",
    "recommendations = [\n",
    "    \"Configure data loaders with optimal block sizes identified above\",\n",
    "    \"Use sequential access patterns when possible (sort data, use columnar formats)\",\n",
    "    \"Consider multi-threaded data loading based on scaling results\",\n",
    "    \"For large datasets, test memory mapping vs regular I/O\",\n",
    "    \"Monitor storage utilization during ML training to identify bottlenecks\",\n",
    "    \"Consider NVMe caching for frequently accessed data\",\n",
    "    \"Use compression if CPU is available and storage is slow\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(\"\\n‚úÖ I/O Microbenchmarks Complete!\")\n",
    "print(f\"üìä Check ../results/ for detailed CSV files and analysis plots\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
