{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cedd50b",
   "metadata": {},
   "source": [
    "# 00 - Environment Check\n",
    "\n",
    "This notebook validates the environment for GPU storage ML experiments across different platforms:\n",
    "- **Local development**: Single node with GPU support\n",
    "- **Google Colab**: Free GPU instances\n",
    "- **AWS SageMaker**: Unified Studio integration\n",
    "- **EMR Spark**: Distributed processing\n",
    "\n",
    "## Key Checks\n",
    "1. Python environment and core libraries\n",
    "2. GPU availability and specifications\n",
    "3. Storage and memory characteristics\n",
    "4. Cloud platform detection\n",
    "5. Spark configuration readiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7231d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import os\n",
    "import psutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß ENVIRONMENT VALIDATION FOR GPU STORAGE ML PROJECT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic system info\n",
    "print(f\"\\nüìã System Information:\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"   Platform: {platform.platform()}\")\n",
    "print(f\"   Architecture: {platform.machine()}\")\n",
    "print(f\"   CPU cores: {psutil.cpu_count()} ({psutil.cpu_count(logical=False)} physical)\")\n",
    "print(f\"   Memory: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "# Cloud platform detection\n",
    "print(f\"\\n‚òÅÔ∏è Cloud Platform Detection:\")\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "is_sagemaker = os.path.exists('/opt/ml')\n",
    "is_emr = os.path.exists('/etc/hadoop')\n",
    "print(f\"   Google Colab: {'‚úÖ Yes' if is_colab else '‚ùå No'}\")\n",
    "print(f\"   AWS SageMaker: {'‚úÖ Yes' if is_sagemaker else '‚ùå No'}\")\n",
    "print(f\"   EMR Cluster: {'‚úÖ Yes' if is_emr else '‚ùå No'}\")\n",
    "print(f\"   Local Environment: {'‚úÖ Yes' if not (is_colab or is_sagemaker or is_emr) else '‚ùå No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU detection and specifications\n",
    "print(f\"\\nüéÆ GPU Configuration:\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"   PyTorch: {torch.__version__}\")\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"   CUDA available: {'‚úÖ Yes' if cuda_available else '‚ùå No'}\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        device_count = torch.cuda.device_count()\n",
    "        print(f\"   GPU count: {device_count}\")\n",
    "        for i in range(device_count):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            memory_gb = props.total_memory / (1024**3)\n",
    "            print(f\"   GPU {i}: {props.name} ({memory_gb:.1f} GB)\")\n",
    "            print(f\"           Compute capability: {props.major}.{props.minor}\")\n",
    "        \n",
    "        # Test basic GPU operation\n",
    "        try:\n",
    "            x = torch.rand(1000, 1000, device='cuda')\n",
    "            y = torch.matmul(x, x.T)\n",
    "            print(f\"   ‚úÖ Basic GPU operations working\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è GPU operation test failed: {e}\")\n",
    "    else:\n",
    "        print(f\"   ‚ÑπÔ∏è Running in CPU-only mode\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(f\"   ‚ùå PyTorch not available\")\n",
    "\n",
    "# NVIDIA tools check\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"   ‚úÖ nvidia-smi available\")\n",
    "        # Extract basic info from nvidia-smi\n",
    "        lines = result.stdout.split('\\n')\n",
    "        for line in lines:\n",
    "            if 'Driver Version' in line:\n",
    "                print(f\"   Driver: {line.split('Driver Version: ')[1].split()[0]}\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"   ‚ùå nvidia-smi not working\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"   ‚ùå nvidia-smi not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "storage_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage characteristics\n",
    "print(f\"\\nüíæ Storage Analysis:\")\n",
    "cwd = Path.cwd()\n",
    "disk_usage = psutil.disk_usage(cwd)\n",
    "print(f\"   Working directory: {cwd}\")\n",
    "print(f\"   Available space: {disk_usage.free / (1024**3):.1f} GB\")\n",
    "print(f\"   Total space: {disk_usage.total / (1024**3):.1f} GB\")\n",
    "\n",
    "# Check for different storage types\n",
    "storage_paths = {\n",
    "    'Local NVMe/SSD': ['/dev/nvme*', '/dev/ssd*'],\n",
    "    'Network mounts': ['/mnt/*', '/net/*'],\n",
    "    'Cloud storage': ['/gcs/*', '/s3/*', '/efs/*']\n",
    "}\n",
    "\n",
    "import glob\n",
    "for storage_type, patterns in storage_paths.items():\n",
    "    found = []\n",
    "    for pattern in patterns:\n",
    "        found.extend(glob.glob(pattern))\n",
    "    if found:\n",
    "        print(f\"   {storage_type}: {found[:3]}{'...' if len(found) > 3 else ''}\")\n",
    "\n",
    "# Test write performance for quick assessment\n",
    "test_file = cwd / 'temp_write_test.dat'\n",
    "try:\n",
    "    import time\n",
    "    data = b'x' * (10 * 1024 * 1024)  # 10MB\n",
    "    start = time.time()\n",
    "    with open(test_file, 'wb') as f:\n",
    "        f.write(data)\n",
    "        f.flush()\n",
    "        os.fsync(f.fileno())\n",
    "    write_time = time.time() - start\n",
    "    write_speed = len(data) / (1024**2) / write_time\n",
    "    print(f\"   Sequential write speed: {write_speed:.1f} MB/s\")\n",
    "    test_file.unlink()  # cleanup\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Write test failed: {e}\")\n",
    "    if test_file.exists():\n",
    "        test_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "libs_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core library availability\n",
    "print(f\"\\nüìö Library Availability:\")\n",
    "libraries = {\n",
    "    'Data Science': ['pandas', 'numpy', 'matplotlib', 'scipy', 'scikit-learn'],\n",
    "    'Big Data': ['pyspark', 'pyarrow'],\n",
    "    'ML Acceleration': ['torch', 'tensorflow'],\n",
    "    'Data Loading': ['webdataset', 'ffcv'],\n",
    "    'Networking': ['ucx-py']\n",
    "}\n",
    "\n",
    "for category, libs in libraries.items():\n",
    "    print(f\"   {category}:\")\n",
    "    for lib in libs:\n",
    "        try:\n",
    "            if lib == 'torch':\n",
    "                import torch\n",
    "                version = torch.__version__\n",
    "            elif lib == 'tensorflow':\n",
    "                import tensorflow as tf\n",
    "                version = tf.__version__\n",
    "            else:\n",
    "                module = __import__(lib.replace('-', '_'))\n",
    "                version = getattr(module, '__version__', 'unknown')\n",
    "            print(f\"     ‚úÖ {lib}: {version}\")\n",
    "        except ImportError:\n",
    "            print(f\"     ‚ùå {lib}: not installed\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ö†Ô∏è {lib}: error ({e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark configuration check\n",
    "print(f\"\\n‚ö° Spark Configuration:\")\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark\n",
    "    print(f\"   PySpark: {pyspark.__version__}\")\n",
    "    \n",
    "    # Test basic Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"EnvironmentTest\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(f\"   ‚úÖ Spark session created successfully\")\n",
    "    print(f\"   Spark version: {spark.version}\")\n",
    "    print(f\"   Default parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "    \n",
    "    # Test basic operation\n",
    "    df = spark.range(1000).toDF(\"number\")\n",
    "    count = df.count()\n",
    "    print(f\"   ‚úÖ Basic Spark operation successful (count: {count})\")\n",
    "    \n",
    "    spark.stop()\n",
    "    \n",
    "    # Check for RAPIDS/GPU support\n",
    "    try:\n",
    "        import cudf\n",
    "        print(f\"   ‚úÖ RAPIDS cuDF available: {cudf.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"   ‚ùå RAPIDS cuDF not available\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Spark test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment summary and recommendations\n",
    "print(f\"\\nüìä ENVIRONMENT SUMMARY:\")\n",
    "print(f\"=\" * 40)\n",
    "\n",
    "# Create environment profile\n",
    "env_profile = {\n",
    "    'platform': {\n",
    "        'local': not (is_colab or is_sagemaker or is_emr),\n",
    "        'colab': is_colab,\n",
    "        'sagemaker': is_sagemaker,\n",
    "        'emr': is_emr\n",
    "    },\n",
    "    'compute': {\n",
    "        'cpu_cores': psutil.cpu_count(),\n",
    "        'memory_gb': round(psutil.virtual_memory().total / (1024**3), 1),\n",
    "        'gpu_available': cuda_available if 'cuda_available' in locals() else False\n",
    "    },\n",
    "    'capabilities': {\n",
    "        'basic_ml': True,  # We have pandas, numpy etc\n",
    "        'gpu_acceleration': 'cuda_available' in locals() and cuda_available,\n",
    "        'distributed_spark': True,  # PySpark is working\n",
    "        'advanced_loaders': False  # FFCV, WebDataset may not be available\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ Recommended Experiment Configuration:\")\n",
    "if env_profile['platform']['colab']:\n",
    "    print(f\"   üì± Google Colab detected:\")\n",
    "    print(f\"     - Use free GPU for training experiments\")\n",
    "    print(f\"     - Limited to ~12GB memory, optimize batch sizes\")\n",
    "    print(f\"     - Mount Google Drive for large datasets\")\n",
    "elif env_profile['platform']['sagemaker']:\n",
    "    print(f\"   üî¨ AWS SageMaker detected:\")\n",
    "    print(f\"     - Use SageMaker Studio for integrated experience\")\n",
    "    print(f\"     - S3 integration for large-scale storage experiments\")\n",
    "    print(f\"     - Consider SageMaker Processing for distributed jobs\")\n",
    "elif env_profile['platform']['emr']:\n",
    "    print(f\"   üî• EMR Cluster detected:\")\n",
    "    print(f\"     - Focus on distributed Spark experiments\")\n",
    "    print(f\"     - HDFS and S3 storage comparisons\")\n",
    "    print(f\"     - Multi-node GPU coordination if available\")\n",
    "else:\n",
    "    print(f\"   üñ•Ô∏è Local Environment:\")\n",
    "    print(f\"     - Full control over storage configurations\")\n",
    "    print(f\"     - Test different storage backends (NVMe, NFS, object)\")\n",
    "    print(f\"     - Single-node optimizations\")\n",
    "\n",
    "if env_profile['compute']['gpu_available']:\n",
    "    print(f\"   üéÆ GPU acceleration enabled - prioritize GPU-bound experiments\")\n",
    "else:\n",
    "    print(f\"   üíª CPU-only mode - focus on I/O and data pipeline experiments\")\n",
    "\n",
    "print(f\"\\n‚úÖ Environment check complete! Ready for GPU storage ML experiments.\")\n",
    "\n",
    "# Save environment profile for later use\n",
    "with open('../results/environment_profile.json', 'w') as f:\n",
    "    json.dump(env_profile, f, indent=2)\n",
    "print(f\"üìÑ Environment profile saved to results/environment_profile.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
