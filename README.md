ğŸš€ GPU-Centric Storage for Machine Learning
Comprehensive Benchmarks & Architecture Exploration

A practical benchmarking and experimentation framework designed to analyze storage bottlenecks in GPU-accelerated machine learning workflows.

This repository provides reproducible experiments, performance comparisons, and optimization strategies for improving end-to-end ML pipeline efficiency.

ğŸ¯ Project Objective

Modern ML workloads are often GPU-bound in theory but I/O-bound in practice.
This project explores how storage systems impact:

Training throughput

ETL performance

Inference latency

GPU utilization efficiency

End-to-end pipeline performance

The goal is to identify bottlenecks and propose GPU-centric storage optimization strategies.

ğŸ”¬ What This Repository Includes
ğŸ“Š Benchmarking Modules

I/O microbenchmarks (sequential vs random access)

CPU vs GPU Spark ETL comparisons

Training data loader performance analysis

Inference latency evaluation

Caching and storage tier experiments

Arrow Flight performance testing

ğŸ§ª Reproducible Experiments

Structured notebook workflow

Configurable Spark environments (CPU & GPU)

Synthetic dataset generation tools

Automated benchmarking scripts

ğŸ“ˆ Performance Analysis

Throughput comparison plots

GPU idle time analysis

Memory usage tracking

Cost-performance tradeoff insights

ğŸ“š Notebook Workflow

Run notebooks in order for full analysis:

00_environment_check â€“ System validation

01_io_microbenchmarks â€“ Storage performance testing

02_spark_etl_cpu_vs_gpu â€“ ETL comparison

03_training_throughput â€“ Data loading optimization

04_inference_latency â€“ Serving performance

05_caching_strategies â€“ Storage tier evaluation

06_arrow_flight â€“ High-throughput data serving

ğŸŒ Supported Platforms

Local development environments

Google Colab

AWS SageMaker

EMR Spark clusters

ğŸ› ï¸ Technology Stack
ML & Processing Frameworks

PyTorch

Spark

RAPIDS cuDF

NVIDIA DALI

FFCV

Storage & Data Formats

Parquet

ORC

Apache Iceberg

Delta Lake

Apache Arrow

Arrow Flight

ğŸ—ï¸ Advanced Experiments

Data lakehouse performance analysis

Snapshot-based reproducibility workflows

GPU memory optimization experiments

Distributed data serving performance

ğŸ“ Repository Structure
gpu_storage_ml_project/
â”œâ”€â”€ notebooks/        # Interactive benchmarking notebooks
â”œâ”€â”€ src/bench/        # Benchmark utilities
â”œâ”€â”€ configs/          # Spark & storage configurations
â”œâ”€â”€ scripts/          # Automation scripts
â”œâ”€â”€ results/          # Benchmark outputs
â”œâ”€â”€ experiments/      # Experiment logs
â””â”€â”€ data/             # Generated datasets
ğŸ¯ Key Research Themes

Quantifying I/O bottlenecks in ML pipelines

Evaluating GPU acceleration impact on ETL

Comparing storage formats for training workloads

Designing GPU-centric storage architectures

Improving GPU utilization through data optimization

ğŸš€ Getting Started

Clone the repository

Set up the environment

Run 00_environment_check

Execute notebooks sequentially

Analyze benchmark results